{
  "_instructions": "Add your work experience here. Copy the experience object for each job.",
  "sectionTitle": "Professional Experience",
  "experiences": [
    {
      "title": "Data Engineer",
      "company": "Baylor Scott & White Health",
      "period": "Aug 2024 - Present",
      "location": "TX, USA",
      "description": "Designed and implemented a scalable, real-time healthcare data pipeline to support patient risk stratification and clinical decision support, processing high-volume transactional data from EHRs, lab systems, and clinical applications.",
      "responsibilities": [
        "Designed and implemented a scalable, real-time healthcare data pipeline to support patient risk stratification and clinical decision support, processing high-volume transactional data from EHRs, lab systems, and clinical applications",
        "Leveraged Apache Kafka for real-time ingestion and stream processing of patient encounters, vitals, lab events, and medication data, enabling low-latency analytics and near real-time clinical insights",
        "Optimized pipeline performance, achieving a 40% reduction in processing time compared to legacy batch workflows, improving the timeliness of patient risk alerts and operational reporting",
        "Created Python-based API integrations to ingest external healthcare data sources (labs, clinical registries, third-party risk scores) and merged them with internal EHR datasets to enhance patient risk assessments",
        "Performed advanced feature engineering on patient and encounter data, creating features such as visit frequency, medication adherence trends, abnormal lab patterns, length-of-stay indicators, and comorbidity risk scores using time-series analysis and feature scaling",
        "Implemented robust data cleansing and validation logic during the ETL process, handling missing clinical fields, inconsistent coding (ICD, CPT), and outlier values to ensure high-quality, analytics-ready healthcare data",
        "Developed incremental and batch processing pipelines in Azure Databricks to support near real-time patient risk analytics and daily population health reporting, reducing data processing latency by 15% and improving data availability for clinical decision-making",
        "Designed and mapped healthcare data into a centralized ER data model, capturing patient demographics, encounters, diagnoses, procedures, medications, and outcomes to support enterprise analytics and reporting",
        "Executed prompt-engineered GenAI pipelines to generate contextual explanations for patient risk scores (readmission risk, chronic disease risk), enabling explainable AI outputs for clinicians and care managers",
        "Built interactive Power BI dashboards to visualize key healthcare metrics including patient risk scores, readmission probability, utilization trends, and care quality indicators, enabling clinicians and administrators to make data-driven decisions",
        "Utilized Teradata SQL for efficient healthcare data transformation, aggregation, and performance optimization, ensuring clean and structured datasets for downstream analytics, dashboards, and machine learning models"
      ]
    },
    {
      "title": "Data Engineer",
      "company": "Accenture",
      "period": "May 2020 - Jul 2023",
      "location": "India",
      "description": "Integrated data from core banking systems, transaction ledgers, credit bureaus, and customer management systems into a centralized financial data warehouse using Apache NiFi, enabling seamless and scalable ETL workflows.",
      "responsibilities": [
        "Integrated data from core banking systems, transaction ledgers, credit bureaus, and customer management systems into a centralized financial data warehouse using Apache NiFi, enabling seamless and scalable ETL workflows",
        "Used MongoDB to store semi-structured financial data such as customer profiles, transaction histories, loan details, and credit events, enabling flexible schema evolution and efficient querying of diverse datasets",
        "Leveraged Python (Pandas, NumPy) to preprocess and cleanse raw financial data, handling missing values (25%), outliers (15%), and data normalization (18%), improving overall data quality and reliability by 70% for downstream analytics",
        "Developed Tableau dashboards to visualize key financial metrics including customer segmentation, credit risk indicators, portfolio performance, transaction trends, and default rates, enabling near real-time decision-making for risk and finance teams",
        "Employed Hive for batch processing and analysis of historical financial data such as transactions, loan records, payment histories, and account balances, generating reports on risk trends, portfolio performance, and customer behavior",
        "Applied MapReduce to perform large-scale aggregations, including average loan exposure, delinquency rates, customer lifetime value, and transaction volumes, deriving actionable insights from high-volume financial data",
        "Implemented Data Vault modeling using hubs, links, and satellites to capture historical financial data in a scalable and auditable manner, managing relationships between customers, accounts, transactions, and financial products, improving data management efficiency by 10%",
        "Utilized Hadoop HDFS to store large-scale financial datasets such as transaction logs, customer histories, and portfolio data, ensuring 32% higher availability and 20% improved fault tolerance, optimizing storage and access across the platform",
        "Built classification models (Logistic Regression, Random Forest) to predict credit risk, loan default probability, and customer churn, using features such as transaction behavior, credit utilization, repayment history, and demographic attributes",
        "Designed Star Schema for financial reporting and analytics, where fact tables captured transactional data and dimension tables stored descriptive attributes (customers, accounts, products, branches), enabling efficient querying and BI reporting",
        "Integrated CI/CD rollback mechanisms into financial data pipelines to ensure that failed deployments (incorrect transformations or data integrity issues) could be reverted instantly to stable versions, preventing downtime and ensuring data consistency for regulatory and business reporting"
      ]
    },
    {
      "title": "Project Analyst",
      "company": "Larsen & Toubro (L&T)",
      "period": "Aug 2017 - May 2020",
      "location": "India",
      "description": "Analyzed large-scale project execution, procurement, and site operations data across multiple L&T infrastructure projects to track schedule adherence, cost variance, and resource utilization, enabling proactive project controls.",
      "responsibilities": [
        "Analyzed large-scale project execution, procurement, and site operations data across multiple L&T infrastructure projects to track schedule adherence, cost variance, and resource utilization, enabling proactive project controls",
        "Built automated dashboards in Power BI and Excel to monitor key KPIs such as planned vs actual progress, budget utilization, equipment downtime, and subcontractor performance, improving management visibility and decision-making",
        "Developed data-driven variance and root-cause analysis to identify delays caused by material shortages, labor inefficiencies, and equipment idle time, supporting corrective action planning and schedule recovery",
        "Collaborated with project managers, site engineers, procurement, and finance teams to translate analytical insights into operational improvements, reducing project overruns and improving coordination across departments",
        "Standardized reporting workflows and operational KPIs, reducing manual reporting effort by 30-35% and improving accuracy and timeliness of monthly project performance reviews by 25%",
        "Wrote complex SQL queries (JOINs, CTEs, subqueries) to analyze project schedules, cost breakdowns, resource allocation, and vendor performance across multiple construction projects",
        "Performed data reconciliation and validation between finance systems, ERP records, and project management tools to ensure accuracy in cost tracking, revenue recognition, and variance reporting",
        "Ensured timely delivery of analytics deliverables by monitoring sprint velocity and issue resolution metrics"
      ]
    }
  ]
}